{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b83cff",
   "metadata": {},
   "source": [
    "# Phishing Email Dataset ‚Äì Cleaning & Split\n",
    "This notebook cleans the raw CSV, performs a **stratified train/val/test split**, and optionally builds a token **vocabulary** for the CNN‚ÄëBiLSTM model.\n",
    "\n",
    "It produces JSONL files ready for:\n",
    "- **DeBERTa‚Äëv3‚Äësmall fine‚Äëtuning** (uses raw cleaned text)\n",
    "- **CNN‚ÄëBiLSTM** (uses same text + vocab)\n",
    "\n",
    "‚ö†Ô∏è **Update `DATA_PATH` below if your CSV lives elsewhere.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcf366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, html, unicodedata, collections\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Paths & constants\n",
    "DATA_PATH = Path('data/raw/phishing_emails.csv')  # <-- change if needed\n",
    "OUTPUT_DIR = Path('data/processed')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VOCAB_SIZE = 20000\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41348614",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f'Loaded {len(df):,} rows')\n",
    "print(df.head())\n",
    "# Expect columns like `text` and `label` (0=legit,1=phish). Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_email(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    # Remove HTML\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text(' ', strip=True)\n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "    # Normalise accents ‚Üí ASCII\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "    # Lowercase & collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text.lower()).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text'].astype(str).apply(clean_email)\n",
    "df = df.rename(columns={'label': 'target'})\n",
    "df = df[['text_clean', 'target']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, tmp_df = train_test_split(\n",
    "    df, test_size=0.30, stratify=df['target'], random_state=RANDOM_SEED)\n",
    "val_df, test_df = train_test_split(\n",
    "    tmp_df, test_size=0.50, stratify=tmp_df['target'], random_state=RANDOM_SEED)\n",
    "print({n: len(d) for n, d in [('train',train_df), ('val',val_df), ('test',test_df)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, d in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
    "    out_path = OUTPUT_DIR / f'{split}.jsonl'\n",
    "    d.rename(columns={'text_clean': 'text'})[['text', 'target']]\n",
    "      .to_json(out_path, orient='records', lines=True, force_ascii=False)\n",
    "    print(f'Saved {out_path} ({len(d)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4678eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_tokenize(text):\n",
    "    return re.findall(r\"\\b\\w[\\w'-]*\\b\", text)\n",
    "\n",
    "counter = collections.Counter()\n",
    "for t in tqdm(train_df['text_clean'], desc='Building vocab'):\n",
    "    counter.update(basic_tokenize(t))\n",
    "\n",
    "most_common = [w for w, _ in counter.most_common(VOCAB_SIZE-2)]  # reserve PAD/UNK\n",
    "vocab_path = OUTPUT_DIR / 'vocab.txt'\n",
    "with open(vocab_path, 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(['<pad>', '<unk>'] + most_common))\n",
    "print(f'Vocab saved ‚ûú {vocab_path} (size={len(most_common)+2})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce56d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nClass distribution:')\n",
    "for name, d in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
    "    print(name, d['target'].value_counts(normalize=True).round(3).to_dict())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
